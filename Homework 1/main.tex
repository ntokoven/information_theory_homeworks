\documentclass[a4paper,10pt]{article}
\usepackage{solutions}

\begin{document}
%%% fill in your team name and each student's name:
\frontpage{Your team name}{Your names}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{nproblem}{7}{Two coins and a die}
You have two (fair) coins and a (fair) 4-sided die with outcomes $\{1,2,3,4\}$. Let $X$ be the number of heads after flipping the two coins and let $Y$ be the result of rolling the die. Let $Z$ be the average of $X$ and $Y$.
\end{nproblem}

\begin{subproblem}{2}
What is the distribution $P_Z$ of $Z$?
\end{subproblem}

\begin{solution}
...
\end{solution}


\begin{subproblem}{3}
Compute the variances of $X$, $Y$ and $Z$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{2}
You play the following game. If $2X \ge Y$, you win $X^2$ euros and otherwise you lose 1 euro. What is your expected total gain or loss after playing this game $40$ times?
\end{subproblem}

\begin{solution}
...
\end{solution}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{nproblem}{9}{Deriving the weak law of large numbers}
\end{nproblem}
	\begin{subproblem}{2} \textbf{(Markov's inequality)} For any real non-negative random variable $X$ and any $t > 0$, show that
	\[
	P[X \geq t] \leq \frac{\mathbb{E}[X]}{t}\, .
	\]
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{1}
	Exhibit a random variable (which can depend on $t$) that achieves this inequality with equality.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{3} \textbf{(Chebyshev's inequality)} Let $Y$ be a random variable with mean $\mu$ and variance $\sigma^2$. Show that for any $\varepsilon > 0$,
	\[
	P[|Y - \mu| \geq \varepsilon] \leq \frac{\sigma^2}{\varepsilon^2} \, .
	\]
	\textbf{Hint:} Define a random variable $X := (Y - \mu)^2$.
\end{subproblem}

\begin{solution}
...
\end{solution}

\begin{subproblem}{3} (The weak law of large numbers.) Let $Z_1, Z_2, ..., Z_n$ real i.i.d. random variables with mean $\mu = \mathbb{E}[Z_i]$ and variance $\sigma^2 = \mathbb{E}[(Z_i - \mu)^2] < \infty$. Define the random variables $S_n = \frac{1}{n} \sum_{i=1}^n Z_i$. Show that
	\[
	P[|S_n - \mu | \geq \varepsilon] \leq \frac{\sigma^2}{n\varepsilon^2}\, .
	\]
	Thus, $P[|S_n - \mu| \geq \varepsilon] \to 0$ as $n \to \infty$. This is known as the weak law of large numbers (which we will use heavily in Week 03).
\end{subproblem}

\begin{solution}
...
\end{solution}












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{nproblem}{3}{Multiple-choice test}
A multiple-choice exam has 4 choices for each question. A student has studied enough so that the probability she will know the answer to a question is 0.5, the probability that she will be able to eliminate one choice is 0.25, otherwise all 4 choices seem equally plausible. If she knows the answer she will get the question right. If not she has to guess from the 3 or 4 choices.
 
As the teacher you want the test to measure what the student knows. If the student answers a question correctly, what is the probability she knew the answer? Give your answer with three decimals of precision.
\begin{solution}
...
\end{solution}
\end{nproblem}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\lang}{\textit{lang}}
\newcommand{\coll}{\textit{coll}}

\begin{nproblem}{8}{Computing Variational Distance (programming)}
The \emph{total variation distance} between two probability distributions $P$ and $Q$
over the finite alphabet $\mathcal{X}$ is defined as 
\begin{align*}
\| P - Q \| := \frac12 \sum_{x \in \mathcal{X} } | P(x) - Q(x) | 
\end{align*}
This distance measure is symmetric, fulfills the triangle inequality and is normalized, i.e.\ it is 0 iff $P=Q$ and 1 iff $P$ and $Q$ have disjoint support.

The \emph{collision probability} of a distribution $P$ over finite alphabet $\mathcal{X}$ is defined as
\begin{align*}
Coll(P) := \sum_{x \in \mathcal{X}} P(x)^2
\end{align*}

In this exercise, we are going to analyze the letter frequencies of \emph{Alice in
  Wonderland} in five different languages: English, German, Esperanto,
Italian and Finnish. You can find all necessary files here: \url{https://github.com/cschaffner/InformationTheory/tree/master/Problems/HW1}. Hereby, we are going to consider only the 26 English letters (without space) and ignore that languages like German and Finnish have important other letters such as {\"a}, {\"o}, {\"u}. 

For $\lang \in \{ \textrm{eng, ger, esp, ita, fin} \}$, let $P_{\lang}$ be the frequency distribution of the 26 English letters (without space) of Alice in Wonderland.

\begin{subproblem}{2}
Compute all pairwise variational distances $\| P_{\lang} - P_{\lang'} \|$ for $\lang \neq \lang' \in \{ \textrm{eng, ger, esp, ita, fin} \}$. Which two languages are closest, which two are furthest apart in terms of variational distance?

\textbf{Note:} for this exercise and any future programming exercises, you do not have to submit your code. In the pdf that you hand in, describe in a few sentences which general strategy you used (e.g., what quantities did you compute and in what order?), any choices you made (e.g., how did you treat uppercase letters? How did you deal with edge cases?), and any `sanity check' computations you may have performed (e.g., did you check what the variational distance between a text file and itself was?) By adding this information, you may still receive partial credit for your approach, even if your final numerical answer is incorrect.
\end{subproblem}
\begin{solution}
...
\end{solution}

\begin{subproblem}{2}
Compute the five collision probabilities $Coll(P_{\lang})$ for  $\lang \in \{ \textrm{eng, ger, esp, ita, fin} \}$. 

\textbf{Note:} You do not have to submit your code.
\end{subproblem}
\begin{solution}
...
\end{solution}

\begin{subproblem}{1}
 Why is it called collision probability?
\end{subproblem}
\begin{solution}
...
\end{solution}

\begin{subproblem}{2}
  You are \href{https://github.com/cschaffner/InformationTheory/blob/master/Problems/HW1/permuted_cipher.txt}{given the file} {\texttt{permuted\_cipher.txt}} that has been encrypted by (first removing spaces and then) shuffling around the
characters (i.e.\ by applying a permutation cipher). Note that this kind of 
encryption preserves the letter frequencies. Compute the frequency distribution
$P_{\textit{cipher}}$ and figure out which language the original text was by
picking the one that minimizes by the variational distance
$\| P_{\textit{cipher}} - P_{\lang}\|$ with $\lang \in \{
\textrm{eng, ger, esp, ita, fin} \}$ as above.

\textbf{Note:} You do not have to submit your code.
\end{subproblem}
\begin{solution}
...
\end{solution}

\begin{subproblem}{1}
Would you have picked the same language when comparing the collision probability $Coll(P_{\textit{cipher}})$ to the ones above?
\end{subproblem}
\begin{solution}
...
\end{solution}

\end{nproblem}




%Entropy diagram for two variables:
%\begin{center}
%% the optional argument in [] lets you determine the size of the circles
%\entropydiagramXY[2]{$H(X)$}{$H(Y)$}{$H(XY)$}{$H(X|Y)$}{$H(Y|X)$}{$I(X;Y)$}
%\end{center}

\end{document}
